---
title: "Proteogenomics"
output: html_document
author: Thomas Zheng
date: 2025-08-12
---

Welcome to the R Markdown for the Proteogenomics workshop!

```{r}
#Let's first load some libraries that we'll need to use for the first section of our workshop!

if (!requireNamespace("tidyverse")) {
    install.packages("tidyverse")
  }
  library(tidyverse)

if (!requireNamespace("ggpubr")) {
    install.packages("ggpubr")
  }
  library(ggpubr)

if (!requireNamespace("SomaDataIO")) {
    install.packages("SomaDataIO")
  }
  library(SomaDataIO)
```

Note: re-run this code block again to remove the install.packages output!

```{r}
#Let's set our working directory now so that we don't have to worry later!

#setwd(INSERT YOUR DIRECTORY HERE)
```

# Intro to Proteomics

## Somalogic Data Processing

```{r}
example_adat <- read_adat("../data/example_data_v5.0_plasma.adat")

#Do some basic exploration of the dataset

#Find the sex composition of the dataset

#Find the age composition of the dataset

#Are there NAs in the dataset?

#What does seq.10000.28 mean?
getAnalyteInfo(example_adat)


```

Here's the link: <https://menu.somalogic.com/> for easier access!

```{r}
example_olink <- read_delim("../data/20212016_Dube_NPX_2021-11-30.csv", delim = ";")

#Do some basic exploration of the dataset

#How many proteins are in this dataset?



#How many samples are there?



#Are there NAs in the dataset?



```

# Association Testing & Multiplexing

Welcome back! Let's work on our original Somalogic data set.

```{r}
#Our Somalogic dataset has some missing data; we could impute (predict) what the missing values should be but we will just remove them for this example.
rm(example_olink)

#This removes all participants with a missing Age (and coincidentally all participants with a missing Sex phenotype)
dataset <- example_adat %>% filter(!is.na(Age))

#In your actual datasets you might want to also check for missing Sex too!

#Let's do a correlation first!

ggplot(dataset, aes(x = Age, y = seq.10514.5)) + geom_point(alpha = 0.2)
```

Take a look at the y axis, this does not really *mean* anything. The units are arbitrary at the moment.

There's two ways for us to scale this data.

-   We could separate this into high and low (or even quartiles)

-   We could standardize this by subtracting the mean and dividing by the standard deviation

### Discrete sections

Let's try separating this into discrete sections first!

```{r}
#Let's just grab the "important" information right now

df <- dataset %>% select(c("SampleId", "Age","Sex","seq.10514.5", "seq.10464.6"))

#Seperates the proteins into High if they are above the median and Low if they are below
df$HighLow105 <- ifelse(df$seq.10514.5 > median(df$seq.10514.5),"High","Low")

#This allows us to run stats on strings
df$HighLow105 <- as.factor(df$HighLow105)


ggplot(df, aes(x = HighLow105, y = Age)) + geom_boxplot()

```

Hmmm, let's try running a t test to see how significant this association is!

```{r}
t.test(Age ~ HighLow105, df)
```

Wow! So we could report this finding as "Participants with protein levels above the median were on average 6.81 [95% CI: 3.21 to 10.41] years older than the participants with protein levels below the median (p-value = 0.0002645)."

Let's try looking at a protein that is associated with biological sex.

```{r}
#Seperates the proteins into High if they are above the median and Low if they are below
df$HighLow104 <- ifelse(df$seq.10464.6 > median(df$seq.10464.6),"High","Low")

#This allows us to run stats on strings
df$HighLow104 <- as.factor(df$HighLow104)

table(df$HighLow104,df$Sex)
```

Huh, looks like females are more likely to be associated with high protein levels!

We can test this with a chi-squared test!

```{r}
chisq.test(df$HighLow104,df$Sex)
```

Great! This one is less meaningful; but we can say something along the lines of "Female participants were significantly more likely to have higher protein levels (above the median) than Male participants (p-value = 0.006107)."

We can even test quartiles instead of just "High/Low", try running the same t-test and chi-squared tests on the quartiles! Here's an easy way to generate the quartiles!

```{r}
df$quartile105 <- cut(df$seq.10514.5,
                   breaks = quantile(df$seq.10514.5, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE),
                   include.lowest = TRUE,
                   labels = c("Q1", "Q2", "Q3", "Q4"))

table(df$quartile105)

#---Now try and run a t.test on age except compare Q4 and Q1-----


#Note that t-tests don't like it when you have more than 2 groups
  #Error in t.test.formula(Age ~ quartile105, df) : 
  #  grouping factor must have exactly 2 levels

#Report the p-value!


#---Using the code above, change it so that you split up seq.10464.6 into quartiles and compare Sex

#Report the p-value!
```

Okay, but running quartiles leads to us throwing away half of our data! What if we don't discretize our proteins?

### Scaling

There are two types of scaling that we will discuss in this workshop:

Z-score scaling and Inverse Normal Transformations (INT)

There are others that suit different methods or applications but we will just discuss these two for today!

INT is primarily used in GWAS so we'll discuss that during the pQTL section later on!

Z-score scaling transforms a value by subtracting the **mean of the variable** (e.g., that specific protein expression across individuals) and dividing by the **standard deviation**.\

A Z-score of 1 means the value is 1 standard deviation above the mean, making it easier to interpret how extreme or typical an observation is relative to the rest.

Luckily, R has a built in function, `scale()`, which does this for us!

```{r}
#Let's try scaling our seq.10514.5 instead of chunking it!
df <- dataset %>% select(c("SampleId", "Age","Sex","seq.10514.5", "seq.10464.6", "seq.10527.22", "seq.10438.19", "seq.10440.26"))

df$scaled_105 <- scale(df$seq.10514.5)

#We should make this into a factor so that our regression can deal with it
df$Sex <- as.factor(df$Sex)

#Now let's see the correlation

cor(df$Age, df$scaled_105)

#We can even get the p-value like so:

cor.test(df$Age, df$scaled_105)

#Keep in mind, that the correlation is only 0.37, which means that the R^2 is only

cor(df$Age, df$scaled_105)^2

#So Age only explains 14% of the variance found in the protein

ggplot(df, aes(x = Age, y = scaled_105)) + geom_point(alpha = 0.5) + stat_cor() + geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "red") 

#This stat_cor is from the ggpubr package and is really convienent!
```

Great! But correlation testing isn't the *best* way of looking at this information.

Let's go back to the PowerPoint!

### Linear Regression

The function for linear regressions is a simple `lm()`.

```{r}

lm(Age ~ scaled_105, df)

#We can get more information out of the model if we do the following

linear_105 <- lm(scaled_105 ~ Age, df)
summary(linear_105)
```

What does this mean?

Residuals are the difference between the observed value and the line of best fit. We want to make sure they are normally distributed (Median around 0, 1Q and 3Q are close in magnitude to one another)

Coefficents are the actual "meat" of the regression. This tells us that at the y-intercept, the estimated value of our protein is -1.73 SD from the mean AND that for every year older, on average the protein level is 0.03 SD higher.

Most of the time, the intercept isn't meaningful for proteomics since adults and infants have different proteomic profiles and you can't have a 0 year old adult.

The **slope for Age (0.031)** means that for every additional year of age, the protein level (`scaled_105`) increases by **0.031 standard deviations**, on average.

The Std. Error or Standard Error reflects the **uncertainty** or **variability** in the estimated coefficient. A smaller SE means we're more confident in the precision of our estimate.

This is also reflected in our t value (coming from a t-distribution), and the p-value.

Our overall formula would look something like this:

$$
\text{scaled_105} = -1.733 + 0.03097 \times \text{Age}
$$

This also works for Categorical variables such as Sex.

Try running a linear regression on Sex and our other protein in df `seq.10464.6`.

```{r}

#First scale seq.10464.6


#Next, run a basic linear regression comparing Sex to seq.10464.6


#What does SexM mean? 


```

### Multivariate Linear Regression

Normally you will have more variables than just Age and Sex in your model. In statistical modeling, such as in biomedical or epidemiological research, it's common and important to include multiple variables in a linear regression model, such as Age, Sex, and other potential confounders or covariates (BMI, sampling location, genetic ancestry, etc). This can be counterintuitive if you're coming from a machine learning perspective, where adding too many variables (especially irrelevant or redundant ones) can lead to overfitting and reduced generalization performance. The goals differ: statistical models often prioritize *interpretability and causal insight*, while machine learning models typically prioritize *prediction accuracy*.

However, in this dataset, we will just use Age, Sex, and we'll even include an interaction term!

Let's take a look back at `seq.10514.5` but this time let's look the model when we include both Age and Sex as variables.

```{r}
multi_linear_105 <- lm(scaled_105 ~ Age + Sex, df)
summary(multi_linear_105)
```

Okay, lets break down what this means.

```         
scaled_105 ~ Age + Sex, data = df
```

Means we are seeing how both Age AND Sex correlate with the data

```         
Residuals:
    Min      1Q  Median      3Q     Max 
-1.9851 -0.5978 -0.0543  0.4452  4.3063 
```

The residuals still look relatively normally distributed, which is promising

```         
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.784327   0.357859  -4.986 1.59e-06 ***
Age          0.031151   0.006058   5.142 7.86e-07 ***
SexM         0.082932   0.146185   0.567    0.571
```

It looks like Age is still significantly associated with protein levels (sometimes adding a co-variate can increase the p-value; especially if the added co-variate explains more of the variation in the data.

But, it looks like Sex is NOT associated with protein level. In fact, SexM means that our baseline is female; and that changing it from Female to Male, does not significantly change the protein levels.

Let's look at `seq.10527.22` now instead!

```{r}

#Scale seq.10527.22
df$scaled_1052 <- scale(df$seq.10527.22)
#Run a multi-variate linear regression

#Read out the summary

```

You *should* see something like this:

```         
Residuals:
    Min      1Q  Median      3Q     Max 
-0.6154 -0.2467 -0.1013  0.0454 11.7777 
```

Woah, that's a crazy max value, I wonder what's happening here. One way we can check is through using the functions

`qqnorm() and qqline()`

```{r}
#qqnorm(residuals(YOUR_MODEL_HERE))
#qqline(residuals(YOUR_MODEL_HERE))
```

![](images/clipboard-651316143.png)

You should see something like this plot appear. This is known as a Q-Q plot which stands for Quantile-Quantile plot. This specific plot is comparing how a normal distribution looks like compared to the actual distribution of the residuals (keep in mind you can also run this on your actual data too). The line that you see here shows the identity, or where your residuals *should* be if everything was normal.

As you can see here, it looks like we have a couple of outliers, especially at the top. Let's see what it looks like as a plot!

```{r}
ggplot(df, aes(x = Age, y = scaled_1052, colour = Sex)) + geom_point(alpha = 0.2)
```

Well then, I think we found our outlier!

This is actually a bad practice to run your analysis *and then* look for outliers. A better practice would be to fix this right after scaling, many people will either cap out outliers (max of 3 or 4) or will exclude them from the data. Let's try excluding them now.

```{r}
df$scaled_1052 <- ifelse(df$scaled_1052 > 4|df$scaled_1052 < -4, NA, df$scaled_1052)
table(is.na(df$scaled_1052))
```

Looks like we just removed one sample anyways!

Let's re-run our analysis now.

```{r}
multi_linear_1052 <- lm(scaled_1052 ~ Age + Sex, df)
summary(multi_linear_1052)
```

Whoops, so looks like that singular outlier was driving all of the significant associations!

Let's check out another example that actually works. `seq.10440.26`

```{r}
df$scaled_1044 <- scale(df$seq.10440.26)
df$scaled_1044 <- ifelse(df$scaled_1044 > 4|df$scaled_1044 < -4, NA, df$scaled_1044)
multi_linear_1044 <- lm(scaled_1044 ~ Age + Sex, df)
summary(multi_linear_1044)
```

Hmmm, let's check the QQ plot

```{r}
qqnorm(residuals(multi_linear_1044))
qqline(residuals(multi_linear_1044))
```

Still not a perfect fit, in fact; let's check the histogram of our original dataset.

```{r}
hist(df$scaled_1044)
```

Ahh, as you can see, our original data was not normally distributed which can lead to these problems. We'll discuss a way to deal with this - but what this does mean is that while our betas or effect sizes are accurate; our p-values may be inaccurate. Thankfully, we have enough observations that through the law of large numbers, we can still be fairly confident in our p-values (although we will also discuss why p-values are a can of worms in of itself).

```         
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.390404   0.336254  -4.135 5.73e-05 ***
Age          0.027389   0.005704   4.802 3.61e-06 ***
SexM        -0.331562   0.137632  -2.409   0.0171 *  
```

Okay, so what do these coefficients mean?

Well, our model is telling us that for every one year older a participant is, they **on average** have an increased protein level of 0.027 standard deviations. Meanwhile, if they are a male as opposed to a female, then they will be -0.33 standard deviations lower to begin with. You can visualize this as:

$$
\text{scaled_1044} = -1.39 + 0.0274 \times \text{Age } + -0.3316 \text{ if Sex = Male} 
$$

![](images/clipboard-3079017492.png)

![](images/clipboard-1143954026.png)

In this case, our men in our sample would be "Group B" while our women would be "Group A".

### Interaction terms

Okay, now that we tried having more than one variable, what if they interact with each other? For example, what if Age was only associated with protein levels in Men but not Women?

We can change that by adding in the term $Sex*Age$ into our regression.

```{r}
multi_linear_interaction_1044 <- lm(scaled_1044 ~ Age + Sex + Age*Sex, df)
summary(multi_linear_interaction_1044)
```

This is an instance of adding an additional covariate results in the loss of significance for another covariate!

```{r}
#Now try on your own to make a multivariable regression with an interaction term with seq.10438.19

#Also, try without the interaction term and examine what happens to the significance. Why do you think that occurred?

```

## Problems associated with multiple comparisons

Let's look at running a bunch of linear regressions together!

```{r}
#Try and write some code that will
#a) scale AND remove the outliers from every single protein tested
#b) Run a linear regression with Age, Sex, and Age*Sex for each single protein


```

```{r eval=FALSE, include=FALSE}
regression <- dataset %>% select(c("Age","Sex"))
regression_protein <- dataset %>% select(starts_with("seq"))
scaled_protein <- regression_protein %>% mutate_all(~ {
    scaled <- scale(.) %>% as.vector()
    ifelse(scaled > 4 | scaled < -4, NA, scaled)
  })

regression <- cbind(regression, scaled_protein)

rm(regression_protein)
rm(scaled_protein)

metrics <- list()

regression$Sex <- as.factor(regression$Sex)

for(i in 3:163){
  form <- paste0(colnames(regression)[i], " ~ Age + Sex + Age*Sex")
  summ_regression <- lm(form, regression)  
  summary_regression <- summary(summ_regression)
  Age_pval <- summary_regression$coefficients[2, "Pr(>|t|)"]
  Age_beta <- summary_regression$coefficients[2, "Estimate"]
  Age_se <- summary_regression$coefficients[2, "Std. Error"]
  
  Sex_pval <- summary_regression$coefficients[3, "Pr(>|t|)"]
  Sex_beta <- summary_regression$coefficients[3, "Estimate"]
  Sex_se <- summary_regression$coefficients[3, "Std. Error"]
  
  Age_Sex_pval <- summary_regression$coefficients[4, "Pr(>|t|)"]
  Age_Sex_beta <- summary_regression$coefficients[4, "Estimate"]
  Age_Sex_se <- summary_regression$coefficients[4, "Std. Error"]
  
  metrics[[i]] <- list(name = colnames(regression)[i],
                      Age_pval = Age_pval,
                       Age_beta = Age_beta,
                       Age_se = Age_se,
                      Sex_pval = Sex_pval,
                       Sex_beta = Sex_beta,
                       Sex_se = Sex_se,
                      Age_Sex_pval = Age_Sex_pval,
                       Age_Sex_beta = Age_Sex_beta,
                       Age_Sex_se = Age_Sex_se
                      )
}

metrics_df <- bind_rows(metrics)

which.min(metrics_df$Age_pval)
which.min(metrics_df$Sex_pval)
which.min(metrics_df$Age_Sex_pval)

metrics_df$name[138]
metrics_df$name[108]
metrics_df$name[12]

metrics_df$Age_Sig <- metrics_df$Age_pval < 0.05
metrics_df$Sex_Sig <- metrics_df$Sex_pval < 0.05
metrics_df$Age_Sex_Sig <- metrics_df$Age_Sex_pval < 0.05

smaller <- metrics_df[metrics_df$Age_Sig & metrics_df$Sex_Sig,]

smaller

ggplot(regression, aes(x = Age, y = seq.10438.19, colour = Sex)) + geom_point(alpha = 0.2)

```

Now you have a bunch of p-values; how many are significant? We can plot this as a volcano plot, where we use the betas or effect sizes as the x-axis and the p-values as the y-axis.

```{r}
metrics_df <- readRDS("../data/data.rds")

ggplot(metrics_df, aes(x = Age_beta, y = Age_pval)) + geom_point(alpha = 0.5)
```

Normally, you see this inverted and the y-axis scaled to a log axis.

```{r}
ggplot(metrics_df, aes(x = Age_beta, y = -log(Age_pval, base = 10), colour = Age_Sig)) + geom_point(alpha = 0.5)
```

Let's try applying both Bonferroni and FDR.

```{r}
metrics_df$Age_pval_Bon <- p.adjust(metrics_df$Age_pval, method = "bonferroni")

table(metrics_df$Age_pval_Bon < 0.05)
```

Wow, we lost all of our significant results except for the top two!

```{r}
metrics_df$Age_pval_FDR <- p.adjust(metrics_df$Age_pval, method = "fdr")

#How many associations do we keep with FDR?
```

Let's recolour our significant hits using this method instead and plot it!

```{r}
#Add or change a significance column

#Re-draw the ggplot
```

Now let's do the same thing but for Sex and our Age\*Sex interaction

```{r}

```
